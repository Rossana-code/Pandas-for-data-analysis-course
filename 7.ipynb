{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/rossana/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bonjour',\n",
       " 'tout',\n",
       " 'le',\n",
       " 'monde',\n",
       " ',',\n",
       " 'il',\n",
       " 'fait',\n",
       " 'beau',\n",
       " 'aujourdui',\n",
       " 'nous',\n",
       " 'allons',\n",
       " 'sortir',\n",
       " 'cet',\n",
       " 'aprem']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = word_tokenize(u'Bonjour tout le monde , il fait beau aujourdui nous allons sortir cet aprem')\n",
    "token\n",
    "# Ou une phase en Anglais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/rossana/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/rossana/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /Users/rossana/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize , pos_tag , ne_chunk\n",
    "# pos_tag : permet de reconnaitre les lieux et ne_chunk des personnes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON John/NNP)\n",
      "  works/VBZ\n",
      "  at/IN\n",
      "  (ORGANIZATION Linkedin/NNP)\n",
      "  in/IN\n",
      "  (GPE Paris/NNP)\n",
      "  in/IN\n",
      "  (GPE France/NNP))\n"
     ]
    }
   ],
   "source": [
    "token = word_tokenize(u'John works at Linkedin in Paris in France')\n",
    "print(ne_chunk(pos_tag(token)))\n",
    "\n",
    "# GPE : lieu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "say\n",
      "échappé\n",
      "échappé\n"
     ]
    }
   ],
   "source": [
    "#Exemple :\n",
    "from nltk.stem.porter import PorterStemmer # PorterStemmer est lematiseur\n",
    "ps = PorterStemmer()\n",
    "# ps : cette variable contient toutes les librairies d'analyse necessaires\n",
    "\n",
    "print(ps.stem(\"saying\")) # say\n",
    "print(ps.stem(\"échappées\")) # échappé : ramener au masculin\n",
    "print(ps.stem(\"échappée\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'navig'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "snow = SnowballStemmer('french')\n",
    "snow.stem('navigatrice') # navig on a la racine navig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.fr.French at 0x7fdc9ac1f1f0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.load('en_core_web_sm')\n",
    "spacy.load('fr_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "John works at Linkedin in Paris in France. He is very happy to work their"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "doc = nlp(u'John works at Linkedin in Paris in France. He is very happy to work their')\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(John, Linkedin, Paris, France)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.ents # On obtients les éléments clés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple Apple\n",
      "is be\n",
      "looking look\n",
      "at at\n",
      "buying buy\n",
      "U.K. U.K.\n",
      "startup startup\n",
      "for for\n",
      "$ $\n",
      "1 1\n",
      "billion billion\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'Apple is looking at buying U.K. startup for $1 billion')\n",
    "for token in doc:\n",
    "    print(token.text , token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cette DET\n",
      "entreprise NOUN\n",
      "a AUX\n",
      "été AUX\n",
      "achetée VERB\n",
      "1 NUM\n",
      "millions NOUN\n",
      "d' ADP\n",
      "euros NOUN\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('fr_core_news_sm')\n",
    "doc = nlp(u\"Cette entreprise a été achetée 1 millions d'euros \")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text , token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer()\n",
      "--------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Création d'un sac vide\n",
    "vectorizer = CountVectorizer()\n",
    "print(vectorizer)\n",
    "print('--------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['un jour ici et là', 'demain mais pas ici', 'la la la lere']\n",
      "--------------\n"
     ]
    }
   ],
   "source": [
    "# Création d'un corpus\n",
    "corpus = ['un jour ici et là', 'demain mais pas ici', 'la la la lere']\n",
    "print(corpus)\n",
    "print('--------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 9)\t1\n",
      "  (1, 0)\t1\n",
      "  (1, 2)\t1\n",
      "  (1, 7)\t1\n",
      "  (1, 8)\t1\n",
      "  (2, 4)\t3\n",
      "  (2, 5)\t1\n",
      "--------------\n"
     ]
    }
   ],
   "source": [
    "# Calcul du sac\n",
    "vectorizer.fit(corpus)\n",
    "bow = vectorizer.transform(corpus)\n",
    "print(bow)\n",
    "print('--------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['demain' 'et' 'ici' 'jour' 'la' 'lere' 'là' 'mais' 'pas' 'un']\n"
     ]
    }
   ],
   "source": [
    "feature_names = vectorizer.get_feature_names_out() # Utilisation de get_feature_names_out()\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 0, 0, 1, 0, 0, 1],\n",
       "       [1, 0, 1, 0, 0, 0, 0, 1, 1, 0],\n",
       "       [0, 0, 0, 0, 3, 1, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Binairisation\n",
    "\n",
    "bow.toarray() # presence ou absece d'un mot dans un document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   demain  et  ici  jour  la  lere  là  mais  pas  un\n",
      "0       0   1    1     1   0     0   1     0    0   1\n",
      "1       1   0    1     0   0     0   0     1    1   0\n",
      "2       0   0    0     0   3     1   0     0    0   0\n"
     ]
    }
   ],
   "source": [
    "# Binairisation en format pandas\n",
    "\n",
    "# Convertir bow en array et utiliser get_feature_names_out() pour les noms de colonnes\n",
    "df = pd.DataFrame(bow.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x10 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 12 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer # compter\n",
    "from sklearn.feature_extraction.text import TfidfTransformer # poids\n",
    "\n",
    "corpus = ['un jour ici et là ' , 'demain mais pas la la la ici' , 'la la la la la la lere']\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# comptage\n",
    "counts = vectorizer.fit_transform(corpus)\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.69314718, 1.69314718, 1.28768207, 1.69314718, 1.28768207,\n",
       "       1.69314718, 1.69314718, 1.69314718, 1.69314718, 1.69314718])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calcul TFIDF à partir du comptage\n",
    "transformer = TfidfTransformer()\n",
    "transformer.fit_transform(counts)\n",
    "# IFIDF diminue la ponderatin des presents dans plusieurs documents.\n",
    "# Exemple les mots \"ici\" & \"la\"\n",
    "\n",
    "transformer.idf_ # les poids\n",
    "# len(transformer.idf_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             IDF\n",
      "demain  1.693147\n",
      "et      1.693147\n",
      "ici     1.287682\n",
      "jour    1.693147\n",
      "la      1.287682\n",
      "lere    1.693147\n",
      "là      1.693147\n",
      "mais    1.693147\n",
      "pas     1.693147\n",
      "un      1.693147\n"
     ]
    }
   ],
   "source": [
    "# Création du dictionnaire à partir des noms des caractéristiques et des scores IDF\n",
    "dic = dict(zip(vectorizer.get_feature_names_out(), transformer.idf_))\n",
    "\n",
    "# Création du DataFrame à partir du dictionnaire\n",
    "df = pd.DataFrame.from_dict(dic, orient=\"index\", columns=['IDF'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'un jour ici et là  demain mais pas la la la ici la la la la la la lere'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_text = ' '.join(corpus)\n",
    "all_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Only supported for TrueType fonts",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/rossana/Documents/GitHub/Pandas-for-data-analysis-course/7.ipynb Cell 28\u001b[0m line \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/rossana/Documents/GitHub/Pandas-for-data-analysis-course/7.ipynb#X36sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m wordcloud \u001b[39m=\u001b[39m WordCloud(width \u001b[39m=\u001b[39;49m \u001b[39m800\u001b[39;49m, height \u001b[39m=\u001b[39;49m \u001b[39m800\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rossana/Documents/GitHub/Pandas-for-data-analysis-course/7.ipynb#X36sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m background_color \u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mwhite\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rossana/Documents/GitHub/Pandas-for-data-analysis-course/7.ipynb#X36sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m min_font_size \u001b[39m=\u001b[39;49m \u001b[39m10\u001b[39;49m)\u001b[39m.\u001b[39;49mgenerate(all_text)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/wordcloud/wordcloud.py:642\u001b[0m, in \u001b[0;36mWordCloud.generate\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate\u001b[39m(\u001b[39mself\u001b[39m, text):\n\u001b[1;32m    628\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Generate wordcloud from text.\u001b[39;00m\n\u001b[1;32m    629\u001b[0m \n\u001b[1;32m    630\u001b[0m \u001b[39m    The input \"text\" is expected to be a natural text. If you pass a sorted\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[39m    self\u001b[39;00m\n\u001b[1;32m    641\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 642\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate_from_text(text)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/wordcloud/wordcloud.py:624\u001b[0m, in \u001b[0;36mWordCloud.generate_from_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    607\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Generate wordcloud from text.\u001b[39;00m\n\u001b[1;32m    608\u001b[0m \n\u001b[1;32m    609\u001b[0m \u001b[39mThe input \"text\" is expected to be a natural text. If you pass a sorted\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    621\u001b[0m \u001b[39mself\u001b[39;00m\n\u001b[1;32m    622\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    623\u001b[0m words \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocess_text(text)\n\u001b[0;32m--> 624\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate_from_frequencies(words)\n\u001b[1;32m    625\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/wordcloud/wordcloud.py:453\u001b[0m, in \u001b[0;36mWordCloud.generate_from_frequencies\u001b[0;34m(self, frequencies, max_font_size)\u001b[0m\n\u001b[1;32m    451\u001b[0m     font_size \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheight\n\u001b[1;32m    452\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 453\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate_from_frequencies(\u001b[39mdict\u001b[39;49m(frequencies[:\u001b[39m2\u001b[39;49m]),\n\u001b[1;32m    454\u001b[0m                                    max_font_size\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mheight)\n\u001b[1;32m    455\u001b[0m     \u001b[39m# find font sizes\u001b[39;00m\n\u001b[1;32m    456\u001b[0m     sizes \u001b[39m=\u001b[39m [x[\u001b[39m1\u001b[39m] \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayout_]\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/wordcloud/wordcloud.py:511\u001b[0m, in \u001b[0;36mWordCloud.generate_from_frequencies\u001b[0;34m(self, frequencies, max_font_size)\u001b[0m\n\u001b[1;32m    508\u001b[0m transposed_font \u001b[39m=\u001b[39m ImageFont\u001b[39m.\u001b[39mTransposedFont(\n\u001b[1;32m    509\u001b[0m     font, orientation\u001b[39m=\u001b[39morientation)\n\u001b[1;32m    510\u001b[0m \u001b[39m# get size of resulting text\u001b[39;00m\n\u001b[0;32m--> 511\u001b[0m box_size \u001b[39m=\u001b[39m draw\u001b[39m.\u001b[39;49mtextbbox((\u001b[39m0\u001b[39;49m, \u001b[39m0\u001b[39;49m), word, font\u001b[39m=\u001b[39;49mtransposed_font, anchor\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mlt\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    512\u001b[0m \u001b[39m# find possible places using integral image:\u001b[39;00m\n\u001b[1;32m    513\u001b[0m result \u001b[39m=\u001b[39m occupancy\u001b[39m.\u001b[39msample_position(box_size[\u001b[39m3\u001b[39m] \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmargin,\n\u001b[1;32m    514\u001b[0m                                    box_size[\u001b[39m2\u001b[39m] \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmargin,\n\u001b[1;32m    515\u001b[0m                                    random_state)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/PIL/ImageDraw.py:671\u001b[0m, in \u001b[0;36mImageDraw.textbbox\u001b[0;34m(self, xy, text, font, anchor, spacing, align, direction, features, language, stroke_width, embedded_color)\u001b[0m\n\u001b[1;32m    669\u001b[0m     font \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgetfont()\n\u001b[1;32m    670\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(font, ImageFont\u001b[39m.\u001b[39mFreeTypeFont):\n\u001b[0;32m--> 671\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mOnly supported for TrueType fonts\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    672\u001b[0m mode \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mRGBA\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m embedded_color \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfontmode\n\u001b[1;32m    673\u001b[0m bbox \u001b[39m=\u001b[39m font\u001b[39m.\u001b[39mgetbbox(\n\u001b[1;32m    674\u001b[0m     text, mode, direction, features, language, stroke_width, anchor\n\u001b[1;32m    675\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: Only supported for TrueType fonts"
     ]
    }
   ],
   "source": [
    "wordcloud = WordCloud(width = 800, height = 800,\n",
    "background_color ='white',\n",
    "min_font_size = 10).generate(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wordcloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/rossana/Documents/GitHub/Pandas-for-data-analysis-course/7.ipynb Cell 29\u001b[0m line \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rossana/Documents/GitHub/Pandas-for-data-analysis-course/7.ipynb#X40sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Afficher le nuage de mots\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rossana/Documents/GitHub/Pandas-for-data-analysis-course/7.ipynb#X40sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m plt\u001b[39m.\u001b[39mfigure(figsize \u001b[39m=\u001b[39m (\u001b[39m4\u001b[39m, \u001b[39m4\u001b[39m), facecolor \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/rossana/Documents/GitHub/Pandas-for-data-analysis-course/7.ipynb#X40sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m plt\u001b[39m.\u001b[39mimshow(wordcloud)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rossana/Documents/GitHub/Pandas-for-data-analysis-course/7.ipynb#X40sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m plt\u001b[39m.\u001b[39maxis(\u001b[39m\"\u001b[39m\u001b[39moff\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rossana/Documents/GitHub/Pandas-for-data-analysis-course/7.ipynb#X40sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m plt\u001b[39m.\u001b[39mtight_layout(pad \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wordcloud' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 288x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Afficher le nuage de mots\n",
    "plt.figure(figsize = (4, 4), facecolor = None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad = 0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/rossana/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['la', 'ici', 'un', 'jour', 'et', 'là', 'demain', 'mais', 'pas', 'lere']\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize(all_text)\n",
    "\n",
    "# Calcul de la fréquence des mots\n",
    "fdist = FreqDist(tokens)\n",
    "\n",
    "# Préparation des données pour le graphique\n",
    "words = [word for word, freq in fdist.most_common(20)]\n",
    "print(words)\n",
    "print(\"------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
